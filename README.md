# learnGPT - team 4

## week 1

### `test_6.py` (1)?

> `gpt_v1` & `gpt_2` do not preserve orders  - why?

### `test_6.py` (2)?

> pos_encodings is designed such that each pos is different - how?

### `test_6.py` (3)?

> pos_encodings is designed such that  |PE(pos + k) - PE(pos)| stays constant - why & how?


[hint 1 ](https://www.facebook.com/groups/TensorFlowKR/posts/1580740202267032)| 
--- | 
<img width="692" alt="image" src="https://user-images.githubusercontent.com/56193069/217115798-8bbc5eab-9888-42ea-9215-09fb40bd8939.png"> |



### `test_6.py` (4)?

>  `gpt_v3` does preserve order  - how?

-  what changes did you make to `token_emb`?


## week 2

...

## Contributors



