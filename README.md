# learnGPT - team 4

## week 1

### `test_6.py` (1)?

> `gpt_v1` & `gpt_2` do not preserve orders  - why?

### `test_6.py` (2)?

> pos_encodings is designed such that each pos is different - how?

### `test_6.py` (3)?

> pos_encodings is designed such that  PE(pos + k) - PE(pos) stays constant - why & how?


### `test_6.py` (4)?

>  `gpt_v3` does preserve order  - how?

-  what changes did you make to `token_emb`?


## week 2

...

## Contributors



